# Pythonç¼–ç¨‹æ¥å£å®Œæ•´ç¤ºä¾‹

## ğŸ Python SDKä½¿ç”¨æŒ‡å—

è¿™ä»½æŒ‡å—å±•ç¤ºå¦‚ä½•é€šè¿‡Pythonä»£ç ä¸æ•´ä¸ªçŸ¥è¯†åº“ç³»ç»Ÿäº¤äº’ï¼Œæ–¹ä¾¿å¤§æ¨¡å‹ç”Ÿæˆæ”¹è¿›æ–¹æ¡ˆã€‚

---

## 1ï¸âƒ£ åŸºç¡€é…ç½®ä¸è¿æ¥

### æ–¹å¼Aï¼šç›´æ¥è°ƒç”¨API

```python
import requests
import json
from typing import List, Dict, Any

class KnowledgeBaseClient:
    """çŸ¥è¯†åº“ç®¡ç†ç³»ç»ŸPythonå®¢æˆ·ç«¯"""
    
    def __init__(self, api_url: str = "http://localhost:8000/api/v1"):
        self.api_url = api_url
        self.session = requests.Session()
        self.token = None
    
    def login(self, username: str, password: str) -> bool:
        """ç™»å½•å¹¶è·å–token"""
        response = self.session.post(
            f"{self.api_url}/auth/login",
            json={"username": username, "password": password}
        )
        if response.status_code == 200:
            self.token = response.json()["access_token"]
            self.session.headers.update({"Authorization": f"Bearer {self.token}"})
            return True
        return False
    
    def upload_document(self, file_path: str, file_type: str = "pdf") -> Dict[str, Any]:
        """ä¸Šä¼ æ–‡æ¡£"""
        with open(file_path, 'rb') as f:
            files = {'file': f}
            response = self.session.post(
                f"{self.api_url}/documents/upload",
                files=files
            )
        return response.json()
    
    def confirm_document(self, document_id: int) -> Dict[str, Any]:
        """ç¡®è®¤æ–‡æ¡£"""
        response = self.session.post(
            f"{self.api_url}/documents/confirm/{document_id}"
        )
        return response.json()
    
    def get_pending_reviews(self) -> List[Dict[str, Any]]:
        """è·å–å¾…å®¡æ ¸æ–‡æ¡£"""
        response = self.session.get(
            f"{self.api_url}/review/pending"
        )
        return response.json()["documents"]
    
    def approve_document(self, document_id: int) -> Dict[str, Any]:
        """å®¡æ ¸é€šè¿‡"""
        response = self.session.post(
            f"{self.api_url}/review/approve/{document_id}"
        )
        return response.json()
    
    def reject_document(self, document_id: int, reason: str) -> Dict[str, Any]:
        """å®¡æ ¸æ‹’ç»"""
        response = self.session.post(
            f"{self.api_url}/review/reject/{document_id}",
            json={"reason": reason}
        )
        return response.json()
    
    def query_knowledge_base(
        self,
        query: str,
        top_k: int = 5,
        model: str = "llama2",
        temperature: float = 0.7
    ) -> Dict[str, Any]:
        """æŸ¥è¯¢çŸ¥è¯†åº“"""
        response = self.session.post(
            f"{self.api_url}/query",
            json={
                "query": query,
                "top_k": top_k,
                "model": model,
                "temperature": temperature
            }
        )
        return response.json()

# ä½¿ç”¨ç¤ºä¾‹
if __name__ == "__main__":
    client = KnowledgeBaseClient()
    
    # ç™»å½•ï¼ˆç®¡ç†å‘˜è´¦å·å¯†ç è¯·ä» .env æˆ–ç¯å¢ƒå˜é‡è¯»å–ï¼Œä¸å»ºè®®å†™æ­»åœ¨ä»£ç é‡Œï¼‰
    import os
    client.login(os.getenv("KB_ADMIN_USERNAME", "admin"), os.environ["KB_ADMIN_PASSWORD"])
    
    # ä¸Šä¼ æ–‡æ¡£
    result = client.upload_document("/path/to/document.pdf")
    document_id = result["document_id"]
    print(f"Document preview: {result['preview']}")
    
    # ç¡®è®¤æ–‡æ¡£
    client.confirm_document(document_id)
    
    # æŸ¥è¯¢çŸ¥è¯†åº“
    response = client.query_knowledge_base("Pythonå¼‚æ­¥ç¼–ç¨‹æ˜¯ä»€ä¹ˆï¼Ÿ")
    print(f"Answer: {response['answer']}")
    print(f"Sources: {response['sources']}")
```

---

## 2ï¸âƒ£ æœ¬åœ°LangChainé›†æˆ

### ç›´æ¥ä½¿ç”¨LangChain + Ollama

```python
from langchain.embeddings import HuggingFaceEmbeddings
from langchain.vectorstores import Milvus
from langchain.llms import Ollama
from langchain.chains import RetrievalQA
from langchain.document_loaders import PyPDFLoader, Docx2txtLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from pymilvus import connections
import os

class LocalRAGSystem:
    """æœ¬åœ°RAGç³»ç»Ÿ"""
    
    def __init__(
        self,
        milvus_host: str = "localhost",
        milvus_port: int = 19530,
        ollama_base_url: str = "http://localhost:11434",
        ollama_model: str = "llama2"
    ):
        # åˆå§‹åŒ–å‘é‡æ¨¡å‹
        self.embeddings = HuggingFaceEmbeddings(
            model_name="sentence-transformers/paraphrase-MiniLM-L6-v2",
            model_kwargs={"device": "cpu"}  # æˆ– "cuda"
        )
        
        # åˆå§‹åŒ–LLM
        self.llm = Ollama(
            base_url=ollama_base_url,
            model=ollama_model,
            temperature=0.7
        )
        
        # Milvusé…ç½®
        self.milvus_host = milvus_host
        self.milvus_port = milvus_port
        self.collection_name = "knowledge_base"
        
        # è¿æ¥Milvus
        self._connect_milvus()
    
    def _connect_milvus(self):
        """è¿æ¥Milvus"""
        connections.connect(
            alias="default",
            host=self.milvus_host,
            port=self.milvus_port
        )
    
    def load_pdf(self, pdf_path: str) -> list:
        """åŠ è½½PDFæ–‡ä»¶"""
        loader = PyPDFLoader(pdf_path)
        documents = loader.load()
        return documents
    
    def load_docx(self, docx_path: str) -> list:
        """åŠ è½½DOCXæ–‡ä»¶"""
        loader = Docx2txtLoader(docx_path)
        documents = loader.load()
        return documents
    
    def split_documents(
        self,
        documents: list,
        chunk_size: int = 512,
        chunk_overlap: int = 50
    ) -> list:
        """æ–‡æ¡£åˆ†å‰²"""
        splitter = RecursiveCharacterTextSplitter(
            chunk_size=chunk_size,
            chunk_overlap=chunk_overlap,
            separators=["\n\n", "\n", " ", ""]
        )
        chunks = splitter.split_documents(documents)
        return chunks
    
    def index_documents(self, documents: list, collection_name: str = None):
        """å°†æ–‡æ¡£ç´¢å¼•åˆ°Milvus"""
        if collection_name is None:
            collection_name = self.collection_name
        
        # åˆ›å»ºå‘é‡å­˜å‚¨
        vector_store = Milvus.from_documents(
            documents=documents,
            embedding=self.embeddings,
            collection_name=collection_name,
            connection_args={
                "host": self.milvus_host,
                "port": self.milvus_port
            }
        )
        
        return vector_store
    
    def query(
        self,
        query_text: str,
        top_k: int = 5,
        collection_name: str = None
    ) -> Dict[str, Any]:
        """æŸ¥è¯¢çŸ¥è¯†åº“å¹¶ç”Ÿæˆå›ç­”"""
        if collection_name is None:
            collection_name = self.collection_name
        
        # åˆ›å»ºå‘é‡å­˜å‚¨å®ä¾‹
        vector_store = Milvus(
            embedding_function=self.embeddings,
            collection_name=collection_name,
            connection_args={
                "host": self.milvus_host,
                "port": self.milvus_port
            }
        )
        
        # åˆ›å»ºæ£€ç´¢å™¨
        retriever = vector_store.as_retriever(
            search_kwargs={"k": top_k}
        )
        
        # åˆ›å»ºRAGé“¾
        qa_chain = RetrievalQA.from_chain_type(
            llm=self.llm,
            chain_type="stuff",
            retriever=retriever,
            return_source_documents=True,
            verbose=True
        )
        
        # æ‰§è¡ŒæŸ¥è¯¢
        result = qa_chain({"query": query_text})
        
        # å¤„ç†ç»“æœ
        sources = [
            {
                "content": doc.page_content,
                "metadata": doc.metadata
            }
            for doc in result["source_documents"]
        ]
        
        return {
            "query": query_text,
            "answer": result["result"],
            "sources": sources,
            "model": self.llm.model
        }
    
    def batch_index(self, directory: str):
        """æ‰¹é‡ç´¢å¼•ç›®å½•ä¸­çš„æ‰€æœ‰æ–‡ä»¶"""
        for filename in os.listdir(directory):
            filepath = os.path.join(directory, filename)
            
            if filename.endswith('.pdf'):
                documents = self.load_pdf(filepath)
            elif filename.endswith('.docx'):
                documents = self.load_docx(filepath)
            else:
                continue
            
            chunks = self.split_documents(documents)
            self.index_documents(chunks)
            print(f"Indexed {filename}: {len(chunks)} chunks")

# ä½¿ç”¨ç¤ºä¾‹
if __name__ == "__main__":
    # åˆå§‹åŒ–ç³»ç»Ÿ
    rag = LocalRAGSystem()
    
    # ç´¢å¼•æ–‡æ¡£
    documents = rag.load_pdf("/path/to/document.pdf")
    chunks = rag.split_documents(documents)
    rag.index_documents(chunks)
    
    # æŸ¥è¯¢
    result = rag.query("ä»€ä¹ˆæ˜¯RAGç³»ç»Ÿï¼Ÿ")
    print(f"Answer: {result['answer']}")
    print(f"Sources: {result['sources']}")
```

---

## 3ï¸âƒ£ æ–‡æ¡£å¤„ç†ç¤ºä¾‹

### å®Œæ•´çš„æ–‡æ¡£è§£ææµç¨‹

```python
import PyPDF2
from docx import Document
from typing import List, Tuple
import re

class DocumentParser:
    """æ–‡æ¡£è§£æå·¥å…·"""
    
    @staticmethod
    def parse_pdf(pdf_path: str) -> str:
        """è§£æPDFæ–‡ä»¶"""
        text = ""
        try:
            with open(pdf_path, 'rb') as f:
                reader = PyPDF2.PdfReader(f)
                for page_num in range(len(reader.pages)):
                    page = reader.pages[page_num]
                    text += f"\n--- Page {page_num + 1} ---\n"
                    text += page.extract_text()
        except Exception as e:
            print(f"Error parsing PDF: {e}")
        return text
    
    @staticmethod
    def parse_docx(docx_path: str) -> str:
        """è§£æDOCXæ–‡ä»¶"""
        doc = Document(docx_path)
        text = ""
        
        for para in doc.paragraphs:
            text += para.text + "\n"
        
        # æå–è¡¨æ ¼
        for table in doc.tables:
            for row in table.rows:
                row_text = " | ".join([cell.text for cell in row.cells])
                text += row_text + "\n"
        
        return text
    
    @staticmethod
    def clean_text(text: str) -> str:
        """æ¸…ç†æ–‡æœ¬"""
        # ç§»é™¤å¤šä½™çš„ç©ºç™½
        text = re.sub(r'\s+', ' ', text)
        
        # ç§»é™¤ç‰¹æ®Šå­—ç¬¦ï¼ˆä¿ç•™ä¸­æ–‡ã€è‹±æ–‡ã€æ•°å­—å’ŒåŸºæœ¬æ ‡ç‚¹ï¼‰
        text = re.sub(r'[^\u4e00-\u9fff\w\sï¼Œã€‚ï¼ï¼Ÿï¼›ï¼šï¼ˆï¼‰\n]', '', text)
        
        return text.strip()
    
    @staticmethod
    def extract_metadata(file_path: str) -> dict:
        """æå–æ–‡ä»¶å…ƒæ•°æ®"""
        import os
        from datetime import datetime
        
        stat = os.stat(file_path)
        return {
            "filename": os.path.basename(file_path),
            "file_size": stat.st_size,
            "modified_time": datetime.fromtimestamp(stat.st_mtime).isoformat(),
            "file_type": os.path.splitext(file_path)[1]
        }

class TextSplitter:
    """æ–‡æœ¬åˆ†å‰²å·¥å…·"""
    
    @staticmethod
    def split_by_sentences(text: str, max_length: int = 512) -> List[str]:
        """æŒ‰å¥å­åˆ†å‰²"""
        # æŒ‰å¥å·ã€é—®å·ã€æ„Ÿå¹å·åˆ†å‰²
        sentences = re.split(r'[ã€‚ï¼ï¼Ÿ\n]+', text)
        
        chunks = []
        current_chunk = ""
        
        for sentence in sentences:
            sentence = sentence.strip()
            if not sentence:
                continue
            
            if len(current_chunk) + len(sentence) <= max_length:
                current_chunk += sentence
            else:
                if current_chunk:
                    chunks.append(current_chunk)
                current_chunk = sentence
        
        if current_chunk:
            chunks.append(current_chunk)
        
        return chunks
    
    @staticmethod
    def split_by_size(text: str, chunk_size: int = 512, overlap: int = 50) -> List[str]:
        """æŒ‰å¤§å°åˆ†å‰²ï¼ˆå¸¦é‡å ï¼‰"""
        chunks = []
        start = 0
        
        while start < len(text):
            end = min(start + chunk_size, len(text))
            chunks.append(text[start:end])
            start = end - overlap
        
        return chunks
    
    @staticmethod
    def split_by_sections(text: str) -> Tuple[List[str], List[str]]:
        """æŒ‰ç« èŠ‚åˆ†å‰²"""
        # åŒ¹é…ç« èŠ‚æ ‡é¢˜ï¼ˆå¦‚ ## æ ‡é¢˜ã€1. æ ‡é¢˜ç­‰ï¼‰
        section_pattern = r'^#{1,6}\s+(.+?)$|^\d+\.\s+(.+?)$'
        
        sections = re.split(section_pattern, text, flags=re.MULTILINE)
        
        chunks = []
        headers = []
        
        for i in range(0, len(sections), 3):
            if i + 2 < len(sections):
                header = sections[i + 1] or sections[i + 2]
                content = sections[i + 3] if i + 3 < len(sections) else ""
                
                if content.strip():
                    chunks.append(content.strip())
                    headers.append(header)
        
        return chunks, headers

# ä½¿ç”¨ç¤ºä¾‹
if __name__ == "__main__":
    # è§£ææ–‡æ¡£
    parser = DocumentParser()
    
    # PDFå¤„ç†
    pdf_text = parser.parse_pdf("/path/to/document.pdf")
    clean_text = parser.clean_text(pdf_text)
    
    # æå–å…ƒæ•°æ®
    metadata = parser.extract_metadata("/path/to/document.pdf")
    print(f"Metadata: {metadata}")
    
    # æ–‡æœ¬åˆ†å‰²
    chunks = TextSplitter.split_by_sentences(clean_text, max_length=512)
    print(f"Chunks count: {len(chunks)}")
    
    # æŒ‰ç« èŠ‚åˆ†å‰²
    sections, headers = TextSplitter.split_by_sections(clean_text)
    for header, section in zip(headers, sections):
        print(f"\n{header}:")
        print(f"{section[:100]}...")
```

---

## 4ï¸âƒ£ Promptå·¥ç¨‹ç¤ºä¾‹

```python
from langchain.prompts import PromptTemplate, ChatPromptTemplate
from langchain.chains import LLMChain
from langchain.llms import Ollama

class PromptManager:
    """æç¤ºè¯ç®¡ç†å™¨"""
    
    # é€šç”¨RAGæŸ¥è¯¢æç¤ºè¯
    RAG_PROMPT = """åŸºäºä»¥ä¸‹èƒŒæ™¯ä¿¡æ¯ï¼Œå›ç­”ç”¨æˆ·çš„é—®é¢˜ã€‚å¦‚æœèƒŒæ™¯ä¿¡æ¯ä¸­æ²¡æœ‰ç›¸å…³å†…å®¹ï¼Œè¯·æ˜ç¡®è¯´æ˜ã€‚

èƒŒæ™¯ä¿¡æ¯:
{context}

ç”¨æˆ·é—®é¢˜: {question}

è¯·æä¾›è¯¦ç»†ã€å‡†ç¡®çš„å›ç­”ï¼Œå¹¶åœ¨é€‚å½“çš„åœ°æ–¹å¼•ç”¨èƒŒæ™¯ä¿¡æ¯ã€‚"""

    # å†…å®¹å®¡æ ¸æç¤ºè¯
    REVIEW_PROMPT = """è¯·å®¡æŸ¥ä»¥ä¸‹æ–‡æ¡£å†…å®¹çš„è´¨é‡å’Œé€‚å½“æ€§ã€‚

æ–‡æ¡£å†…å®¹:
{content}

è¯·ä»ä»¥ä¸‹æ–¹é¢è¯„ä¼°ï¼ˆè¯„åˆ†1-10åˆ†ï¼‰:
1. å†…å®¹æ¸…æ™°åº¦
2. ä¿¡æ¯å‡†ç¡®æ€§
3. ä¸“ä¸šæ€§
4. å®‰å…¨æ€§

æœ€åç»™å‡ºå®¡æ ¸æ„è§ï¼ˆé€šè¿‡/æ‹’ç»ï¼‰å’Œå»ºè®®ã€‚"""

    # æ–‡æœ¬æ€»ç»“æç¤ºè¯
    SUMMARY_PROMPT = """è¯·ä¸ºä»¥ä¸‹æ–‡æœ¬ç”Ÿæˆç®€æ´çš„æ€»ç»“ï¼ˆä¸è¶…è¿‡200å­—ï¼‰:

{text}

æ€»ç»“:"""

    # é”®å€¼æå–æç¤ºè¯
    EXTRACTION_PROMPT = """ä»ä»¥ä¸‹æ–‡æœ¬ä¸­æå–å…³é”®ä¿¡æ¯ï¼Œä»¥JSONæ ¼å¼è¿”å›:

{text}

è¯·æå–ä»¥ä¸‹å­—æ®µ: {fields}

JSONç»“æœ:"""

    @staticmethod
    def create_rag_chain(llm: Ollama):
        """åˆ›å»ºRAGæŸ¥è¯¢é“¾"""
        prompt = PromptTemplate(
            input_variables=["context", "question"],
            template=PromptManager.RAG_PROMPT
        )
        
        chain = LLMChain(llm=llm, prompt=prompt)
        return chain
    
    @staticmethod
    def create_review_chain(llm: Ollama):
        """åˆ›å»ºå®¡æ ¸é“¾"""
        prompt = PromptTemplate(
            input_variables=["content"],
            template=PromptManager.REVIEW_PROMPT
        )
        
        chain = LLMChain(llm=llm, prompt=prompt)
        return chain
    
    @staticmethod
    def create_summary_chain(llm: Ollama):
        """åˆ›å»ºæ€»ç»“é“¾"""
        prompt = PromptTemplate(
            input_variables=["text"],
            template=PromptManager.SUMMARY_PROMPT
        )
        
        chain = LLMChain(llm=llm, prompt=prompt)
        return chain
    
    @staticmethod
    def create_extraction_chain(llm: Ollama, fields: str):
        """åˆ›å»ºæå–é“¾"""
        prompt = PromptTemplate(
            input_variables=["text", "fields"],
            template=PromptManager.EXTRACTION_PROMPT
        )
        
        chain = LLMChain(llm=llm, prompt=prompt)
        return chain

# ä½¿ç”¨ç¤ºä¾‹
if __name__ == "__main__":
    llm = Ollama(
        base_url="http://localhost:11434",
        model="llama2"
    )
    
    # RAGæŸ¥è¯¢
    rag_chain = PromptManager.create_rag_chain(llm)
    result = rag_chain.run(
        context="Pythonæ˜¯ä¸€ç§é«˜çº§ç¼–ç¨‹è¯­è¨€...",
        question="Pythoné€‚åˆåšä»€ä¹ˆï¼Ÿ"
    )
    print(f"RAG Response: {result}")
    
    # æ–‡æœ¬æ€»ç»“
    summary_chain = PromptManager.create_summary_chain(llm)
    summary = summary_chain.run(text="è¿™æ˜¯ä¸€ä¸ªå¾ˆé•¿çš„æ–‡æœ¬ï¼Œéœ€è¦æ€»ç»“...")
    print(f"Summary: {summary}")
    
    # ä¿¡æ¯æå–
    extraction_chain = PromptManager.create_extraction_chain(
        llm,
        fields="å§“å,å…¬å¸,èŒä½"
    )
    result = extraction_chain.run(
        text="John Smith is a Software Engineer at Google...",
        fields="å§“å,å…¬å¸,èŒä½"
    )
    print(f"Extracted: {result}")
```

---

## 5ï¸âƒ£ é«˜çº§åŠŸèƒ½ç¤ºä¾‹

### A. å¤šæ¨¡å‹å¯¹æ¯”

```python
from typing import Dict, List

class MultiModelEvaluator:
    """å¤šæ¨¡å‹è¯„ä¼°å™¨"""
    
    def __init__(self, models: List[str]):
        self.models = models
        self.llms = {
            model: Ollama(
                base_url="http://localhost:11434",
                model=model,
                temperature=0.7
            )
            for model in models
        }
    
    def compare_responses(self, query: str) -> Dict[str, str]:
        """å¯¹æ¯”å¤šä¸ªæ¨¡å‹çš„å›ç­”"""
        results = {}
        
        for model_name, llm in self.llms.items():
            try:
                response = llm.invoke(query)
                results[model_name] = response
            except Exception as e:
                results[model_name] = f"Error: {str(e)}"
        
        return results
    
    def evaluate_quality(self, responses: Dict[str, str]) -> Dict[str, float]:
        """è¯„ä¼°å›ç­”è´¨é‡"""
        scores = {}
        
        for model_name, response in responses.items():
            # ç®€å•æŒ‡æ ‡ï¼šé•¿åº¦ã€æ¸…æ™°åº¦ç­‰
            score = min(len(response) / 100, 10)  # ç¤ºä¾‹è¯„åˆ†
            scores[model_name] = score
        
        return scores

# ä½¿ç”¨
evaluator = MultiModelEvaluator(["llama2", "mistral", "neural-chat"])
responses = evaluator.compare_responses("ä»€ä¹ˆæ˜¯äººå·¥æ™ºèƒ½ï¼Ÿ")
scores = evaluator.evaluate_quality(responses)
```

### B. å®æ—¶åé¦ˆä¸ä¼˜åŒ–

```python
class FeedbackLoop:
    """åé¦ˆå¾ªç¯ç³»ç»Ÿ"""
    
    def __init__(self, llm: Ollama):
        self.llm = llm
        self.feedback_history = []
    
    def generate_with_feedback(
        self,
        query: str,
        feedback_required: bool = False
    ) -> Dict[str, Any]:
        """ç”Ÿæˆå›ç­”å¹¶æ”¶é›†åé¦ˆ"""
        
        # ç¬¬ä¸€æ¬¡ç”Ÿæˆ
        response = self.llm.invoke(query)
        
        result = {
            "query": query,
            "response": response,
            "version": 1
        }
        
        if feedback_required:
            # æ”¶é›†ç”¨æˆ·åé¦ˆï¼ˆæ¨¡æ‹Ÿï¼‰
            feedback = {
                "quality": 7,  # 1-10
                "accuracy": 8,
                "clarity": 6,
                "comments": "éœ€è¦æ›´å¤šå…·ä½“ä¾‹å­"
            }
            
            # åŸºäºåé¦ˆä¼˜åŒ–
            improved_response = self._optimize_response(
                response,
                feedback
            )
            
            result["feedback"] = feedback
            result["improved_response"] = improved_response
            result["version"] = 2
        
        self.feedback_history.append(result)
        return result
    
    def _optimize_response(self, response: str, feedback: dict) -> str:
        """åŸºäºåé¦ˆä¼˜åŒ–å›ç­”"""
        optimization_prompt = f"""
        åŸå§‹å›ç­”:
        {response}
        
        åé¦ˆ:
        {feedback}
        
        è¯·æ ¹æ®åé¦ˆæ”¹è¿›å›ç­”:
        """
        
        improved = self.llm.invoke(optimization_prompt)
        return improved

# ä½¿ç”¨
feedback_loop = FeedbackLoop(llm)
result = feedback_loop.generate_with_feedback("è§£é‡Šä»€ä¹ˆæ˜¯é€’å½’ï¼Ÿ", feedback_required=True)
print(f"Version 1: {result['response']}")
print(f"Version 2: {result['improved_response']}")
```

### C. æ‰¹é‡å¤„ç†ä¸ç›‘æ§

```python
from concurrent.futures import ThreadPoolExecutor
import time

class BatchProcessor:
    """æ‰¹é‡å¤„ç†å™¨"""
    
    def __init__(self, rag_system: LocalRAGSystem, max_workers: int = 5):
        self.rag_system = rag_system
        self.max_workers = max_workers
        self.stats = {
            "total": 0,
            "success": 0,
            "failed": 0,
            "avg_time": 0
        }
    
    def process_queries(self, queries: List[str]) -> List[Dict]:
        """æ‰¹é‡å¤„ç†æŸ¥è¯¢"""
        results = []
        times = []
        
        def process_single(query):
            start = time.time()
            try:
                result = self.rag_system.query(query)
                result["status"] = "success"
                result["processing_time"] = time.time() - start
                times.append(result["processing_time"])
                return result
            except Exception as e:
                return {
                    "query": query,
                    "status": "failed",
                    "error": str(e)
                }
        
        with ThreadPoolExecutor(max_workers=self.max_workers) as executor:
            results = list(executor.map(process_single, queries))
        
        # æ›´æ–°ç»Ÿè®¡
        self.stats["total"] = len(queries)
        self.stats["success"] = len([r for r in results if r["status"] == "success"])
        self.stats["failed"] = len([r for r in results if r["status"] == "failed"])
        if times:
            self.stats["avg_time"] = sum(times) / len(times)
        
        return results
    
    def get_stats(self) -> Dict:
        """è·å–å¤„ç†ç»Ÿè®¡"""
        return self.stats

# ä½¿ç”¨
processor = BatchProcessor(rag_system, max_workers=3)
queries = [
    "ä»€ä¹ˆæ˜¯Pythonï¼Ÿ",
    "å¦‚ä½•å­¦ä¹ ç¼–ç¨‹ï¼Ÿ",
    "æœºå™¨å­¦ä¹ æ˜¯ä»€ä¹ˆï¼Ÿ"
]
results = processor.process_queries(queries)
print(f"Stats: {processor.get_stats()}")
```

---

## ğŸ“Š å®Œæ•´å·¥ä½œæµç¤ºä¾‹

```python
async def complete_workflow():
    """å®Œæ•´å·¥ä½œæµæ¼”ç¤º"""
    
    # 1. åˆå§‹åŒ–ç³»ç»Ÿ
    client = KnowledgeBaseClient()
    import os
    client.login(os.getenv("KB_ADMIN_USERNAME", "admin"), os.environ["KB_ADMIN_PASSWORD"])
    
    rag = LocalRAGSystem()
    
    # 2. ä¸Šä¼ å¹¶å¤„ç†æ–‡æ¡£
    print("Step 1: Uploading document...")
    upload_result = client.upload_document("/path/to/document.pdf")
    document_id = upload_result["document_id"]
    
    # 3. ç”¨æˆ·ç¡®è®¤
    print("Step 2: Confirming document...")
    client.confirm_document(document_id)
    
    # 4. ç®¡ç†å‘˜å®¡æ ¸
    print("Step 3: Admin review...")
    pending = client.get_pending_reviews()
    if pending:
        client.approve_document(pending[0]["id"])
    
    # 5. ç´¢å¼•æ–‡æ¡£
    print("Step 4: Indexing document...")
    parser = DocumentParser()
    text = parser.parse_pdf("/path/to/document.pdf")
    splitter = TextSplitter()
    chunks = splitter.split_by_sentences(text)
    
    # ä½¿ç”¨LangChainç›´æ¥ç´¢å¼•
    from langchain.schema import Document
    docs = [Document(page_content=chunk) for chunk in chunks]
    rag.index_documents(docs)
    
    # 6. æŸ¥è¯¢çŸ¥è¯†åº“
    print("Step 5: Querying knowledge base...")
    result = rag.query("æ–‡æ¡£çš„ä¸»è¦å†…å®¹æ˜¯ä»€ä¹ˆï¼Ÿ")
    print(f"Answer: {result['answer']}")
    
    # 7. æ‰¹é‡æŸ¥è¯¢
    print("Step 6: Batch queries...")
    processor = BatchProcessor(rag)
    queries = ["ä»€ä¹ˆæ˜¯RAGï¼Ÿ", "RAGçš„ä¼˜ç‚¹æ˜¯ä»€ä¹ˆï¼Ÿ"]
    batch_results = processor.process_queries(queries)
    print(f"Batch stats: {processor.get_stats()}")

if __name__ == "__main__":
    import asyncio
    asyncio.run(complete_workflow())
```

---

## ğŸ¯ ä¸å¤§æ¨¡å‹çš„äº¤äº’æ¨¡å¼

å½“ä½ éœ€è¦å¤§æ¨¡å‹æ”¹è¿›ç³»ç»Ÿæ—¶ï¼Œå¯ä»¥è¿™æ ·æé—®ï¼š

```python
# ç¤ºä¾‹ï¼šè®©å¤§æ¨¡å‹ä¼˜åŒ–Prompt
improvement_request = """
å½“å‰RAGç³»ç»ŸæŸ¥è¯¢Prompt:
{current_prompt}

é—®é¢˜: 
- å›ç­”ä¸å¤Ÿå‡†ç¡®
- æœªèƒ½å……åˆ†å¼•ç”¨æºæ–‡æ¡£
- å›ç­”è¿‡äºå†—é•¿

è¯·æ”¹è¿›Promptï¼Œä½¿å…¶:
1. æ›´å‡†ç¡®åœ°å›ç­”é—®é¢˜
2. æ¸…æ¥šåœ°æ ‡æ³¨å¼•ç”¨
3. æ§åˆ¶å›ç­”é•¿åº¦åœ¨200å­—ä»¥å†…

æä¾›æ”¹è¿›åçš„Promptä»£ç ã€‚
"""

# ç¤ºä¾‹ï¼šè®©å¤§æ¨¡å‹æ·»åŠ æ–°åŠŸèƒ½
feature_request = """
å½“å‰ç³»ç»Ÿæ¶æ„: {system_architecture}

éœ€è¦æ–°åŠŸèƒ½:
- æ”¯æŒè‡ªåŠ¨ç”Ÿæˆæ–‡æ¡£æ‘˜è¦
- æ”¯æŒå¤šè¯­è¨€æŸ¥è¯¢
- å®æ—¶æ€§èƒ½ç›‘æ§

è¯·ä¸ºè¿™ä¸‰ä¸ªåŠŸèƒ½ï¼š
1. è®¾è®¡Pythonç±»å’Œæ–¹æ³•
2. ç»™å‡ºå…·ä½“å®ç°ä»£ç 
3. è¯´æ˜ä¸ç°æœ‰ç³»ç»Ÿçš„é›†æˆæ–¹å¼
"""
```

è¿™æ ·å¤§æ¨¡å‹å°±èƒ½ç†è§£ä½ çš„ç³»ç»Ÿï¼Œç”Ÿæˆå¯ä»¥ç›´æ¥ä½¿ç”¨æˆ–æœ€å°åŒ–ä¿®æ”¹å°±èƒ½ç”¨çš„ä»£ç ã€‚

---

## ğŸ“š å¸¸ç”¨å¯¼å…¥æ¸…å•

```python
# APIå®¢æˆ·ç«¯
import requests
from requests.auth import HTTPBasicAuth

# LangChain
from langchain.embeddings import HuggingFaceEmbeddings
from langchain.vectorstores import Milvus
from langchain.llms import Ollama
from langchain.chains import RetrievalQA, LLMChain
from langchain.document_loaders import PyPDFLoader, Docx2txtLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.prompts import PromptTemplate
from langchain.schema import Document

# Pymilvus
from pymilvus import connections, Collection

# æ–‡æ¡£å¤„ç†
import PyPDF2
from docx import Document as DocxDocument

# å¹¶å‘å¤„ç†
from concurrent.futures import ThreadPoolExecutor
import asyncio

# å·¥å…·åº“
import json
import re
import os
from typing import Dict, List, Any, Tuple
from datetime import datetime
```

ç¥ä½ ä½¿ç”¨æ„‰å¿«ï¼è¿™ä»½æŒ‡å—åŒ…å«äº†æ‰€æœ‰ä½ éœ€è¦ä¸å¤§æ¨¡å‹æ²Ÿé€šçš„æ ¸å¿ƒç¼–ç¨‹æ¥å£ã€‚
